{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK8u_aGl2sVW"
      },
      "source": [
        "# Coursework 1 - Mathematics for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiWQAg9V2sVY"
      },
      "source": [
        "## CID: insert your CID here\n",
        "\n",
        "**Colab link:** insert colab link here\n",
        "\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YIqCr232sVZ"
      },
      "source": [
        "## Part 1: Quickfire questions [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbbcTvYZ2sVZ"
      },
      "source": [
        "#### Question 1 (True risk / Empirical risk):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aEJdGev2sVZ"
      },
      "source": [
        "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZcTQdB92sVZ"
      },
      "source": [
        "#### Question 3 (Dataset splitting):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHjZ21qA2sVZ"
      },
      "source": [
        "#### Question 4 (Occamâ€™s razor):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyVskoM22sVa"
      },
      "source": [
        "#### Question 5 (Generalisation error):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quzf--OL2sVa"
      },
      "source": [
        "#### Question 6 (Rademacher complexity pt1):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc414RJV2sVa"
      },
      "source": [
        "#### Question 7 (Rademacher complexity pt2):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY3o2MXl2sVa"
      },
      "source": [
        "#### Question 8 (Regularisation term in the loss function):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72DlajIW2sVb"
      },
      "source": [
        "#### Question 9 (Momentum gradient descent):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCwc9wkR2sVb"
      },
      "source": [
        "#### Question 10 (Adam):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXIZAndw2sVb"
      },
      "source": [
        "#### Question 11 (AdaGrad):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_k_g_qr2sVb"
      },
      "source": [
        "#### Question 12 (Decaying Learning Rate):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41dGi5wo2sVb"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 2: Short-ish proofs [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zlbVwj2sVb"
      },
      "source": [
        "\n",
        "### Question 2.1: Bounds on the risk [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDSxbRA2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp_Kr9T32sVb"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.2: On semi-definiteness [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2VWR9iQ2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4p6Fcyy2sVb"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.3: A quick recap of momentum [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1VFKHuZ2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwuPSbvP2sVc"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.4: Convergence proof [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNGw6ku2sVc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV-W82HD2sVc"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 3: A deeper dive into neural network implementations [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qT6HJlNL2sVc"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uogO3SpP2sVc",
        "outputId": "f584ae90-5dfd-4784-f100-9410a12f2ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download datasets\n",
        "train_set_mnist = torchvision.datasets.MNIST(root=\"./data\", download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]));\n",
        "\n",
        "test_set_mnist = torchvision.datasets.MNIST(root=\"./data\",download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),);\n",
        "\n",
        "train_set_cifar = torchvision.datasets.CIFAR10(root=\"./data\", download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]));\n",
        "\n",
        "test_set_cifar = torchvision.datasets.CIFAR10(root=\"./data\",download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),);\n",
        "\n",
        "# Normalizing data:\n",
        "train_set_mnist.data = nn.functional.normalize(train_set_mnist.data.to(float), p=1)\n",
        "test_set_mnist.data = nn.functional.normalize(test_set_mnist.data.to(float), p=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[[0.0283, 0.0292, 0.0299],\n",
            "         [0.0274, 0.0290, 0.0302],\n",
            "         [0.0266, 0.0287, 0.0301],\n",
            "         ...,\n",
            "         [0.0291, 0.0301, 0.0301],\n",
            "         [0.0301, 0.0305, 0.0302],\n",
            "         [0.0313, 0.0311, 0.0303]],\n",
            "\n",
            "        [[0.0289, 0.0299, 0.0308],\n",
            "         [0.0283, 0.0297, 0.0308],\n",
            "         [0.0275, 0.0293, 0.0307],\n",
            "         ...,\n",
            "         [0.0322, 0.0315, 0.0307],\n",
            "         [0.0310, 0.0310, 0.0306],\n",
            "         [0.0319, 0.0318, 0.0315]],\n",
            "\n",
            "        [[0.0292, 0.0301, 0.0308],\n",
            "         [0.0283, 0.0297, 0.0308],\n",
            "         [0.0275, 0.0293, 0.0305],\n",
            "         ...,\n",
            "         [0.0322, 0.0306, 0.0295],\n",
            "         [0.0310, 0.0307, 0.0304],\n",
            "         [0.0316, 0.0318, 0.0315]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0318, 0.0313, 0.0310],\n",
            "         [0.0308, 0.0309, 0.0310],\n",
            "         [0.0298, 0.0305, 0.0309],\n",
            "         ...,\n",
            "         [0.0265, 0.0285, 0.0305],\n",
            "         [0.0277, 0.0291, 0.0306],\n",
            "         [0.0285, 0.0294, 0.0307]],\n",
            "\n",
            "        [[0.0314, 0.0308, 0.0308],\n",
            "         [0.0308, 0.0309, 0.0310],\n",
            "         [0.0298, 0.0305, 0.0309],\n",
            "         ...,\n",
            "         [0.0262, 0.0283, 0.0303],\n",
            "         [0.0271, 0.0286, 0.0302],\n",
            "         [0.0282, 0.0292, 0.0305]],\n",
            "\n",
            "        [[0.0311, 0.0306, 0.0306],\n",
            "         [0.0305, 0.0304, 0.0306],\n",
            "         [0.0295, 0.0303, 0.0307],\n",
            "         ...,\n",
            "         [0.0260, 0.0280, 0.0301],\n",
            "         [0.0271, 0.0286, 0.0302],\n",
            "         [0.0278, 0.0289, 0.0303]]]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n"
          ]
        }
      ],
      "source": [
        "class NormalizedDataSet(Dataset):\n",
        "    def __init__(self, dataset_type, train: bool, normalize: bool, nclasses=10):\n",
        "        match (dataset_type, train):\n",
        "            case (\"mnist\", True):\n",
        "                self.data =  train_set_mnist.data\n",
        "                raw_targets = train_set_mnist.targets\n",
        "                \n",
        "            case (\"mnist\", False):\n",
        "                self.data =  test_set_mnist.data\n",
        "                raw_targets = test_set_mnist.targets\n",
        "\n",
        "            case (\"cifar\", True):\n",
        "                self.data =  train_set_cifar.data\n",
        "                raw_targets = train_set_cifar.targets\n",
        "                \n",
        "            case (\"cifar\", False):\n",
        "                self.data =  test_set_cifar.data\n",
        "                raw_targets = test_set_cifar.targets\n",
        "\n",
        "            case _:\n",
        "                raise ValueError(\"Dataset must be 'mnist' or 'cifar'\")\n",
        "            \n",
        "        self.targets = torch.zeros(len(raw_targets), nclasses, dtype=torch.float32) \n",
        "        for i, t in enumerate(raw_targets):\n",
        "            self.targets[i, int(t)] = 1.                            # Changing the targets into rows with 0 everywhere except the target\n",
        "\n",
        "        self.data = torch.tensor(self.data, dtype=torch.float32)            # Casting to float to prevent future problems.\n",
        "        \n",
        "        if normalize:                                               \n",
        "            self.data = nn.functional.normalize(self.data, p=1)\n",
        "            \n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.targets[index]\n",
        "            \n",
        "def test_dataset():\n",
        "    traindata = NormalizedDataSet(\"cifar\", False, True)\n",
        "    print(traindata[10])\n",
        "\n",
        "test_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QEsK7Qzo2sVd"
      },
      "outputs": [],
      "source": [
        "# Set seed\n",
        "SEED = int('02530622')\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GpBUZ0S2sVd",
        "outputId": "eb397d1d-82be-40b0-e2b5-c743e12a1602"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dim, nclass, width, depth):\n",
        "      super().__init__()\n",
        "      self.dim = dim\n",
        "      self.nclass = nclass\n",
        "      self.width = width\n",
        "      self.depth = depth\n",
        "      self.input_length = np.prod(dim)\n",
        "\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_in = nn.Linear(self.input_length, width)\n",
        "      self.linear_hidden = nn.Linear(width, width)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.linear_out = nn.Linear(width, nclass)\n",
        "\n",
        "    def forward(self, x):\n",
        "      flat_x = self.flatten(x)\n",
        "      lifted_x = self.linear_in(flat_x)\n",
        "\n",
        "      processed_x = lifted_x\n",
        "      for _ in range(self.depth):\n",
        "        processed_x = self.relu(self.linear_hidden(processed_x))\n",
        "\n",
        "      return self.linear_out(processed_x)\n",
        "\n",
        "def test_net(net=None):\n",
        "\n",
        "  mnist_net = Net((28, 28), 10, 16, 2) if net is None else net\n",
        "  sample_index = np.random.randint(10000)\n",
        "\n",
        "  x = train_set_mnist.data[sample_index, :, :]\n",
        "  x = torch.unsqueeze(x, 0)\n",
        "  print(mnist_net(x), train_set_mnist.targets[sample_index])\n",
        "\n",
        "\n",
        "# test_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0h4H6KUOCu2-"
      },
      "outputs": [],
      "source": [
        "class Network():\n",
        "  def __init__(\n",
        "      self,\n",
        "      dataset_type,\n",
        "      width,\n",
        "      depth,\n",
        "      criterion, # Notice we assume that reduction=\"mean\"\n",
        "      optimizer,\n",
        "      batch_size=64,\n",
        "      lr=0.001,\n",
        "      max_epoch=1,\n",
        "      normalize=True\n",
        "      ):\n",
        "\n",
        "    match dataset_type:\n",
        "      case \"mnist\":\n",
        "        dim = (28, 28)\n",
        "        nclass = 10\n",
        "\n",
        "      case \"cifar\":\n",
        "        dim = (32, 32, 3)\n",
        "        nclass = 10\n",
        "\n",
        "      case _:\n",
        "        raise ValueError(\"Dataset must be 'mnist' or 'cifar'\")\n",
        "\n",
        "    self.trainset = NormalizedDataSet(dataset_type, train=True, normalize=normalize)\n",
        "    self.testset = NormalizedDataSet(dataset_type, train=False, normalize=normalize)\n",
        "    self.trainloader, self.testloader = self._loading_data(batch_size, self.trainset, self.testset)\n",
        "\n",
        "    self.net = Net(dim, nclass, width, depth)\n",
        "    \n",
        "    self.lr = lr\n",
        "    self.max_epoch = max_epoch\n",
        "    self.optimizer = optimizer(self.net.parameters(), lr=self.lr)\n",
        "    self.criterion = criterion\n",
        "\n",
        "\n",
        "  def _loading_data(self, batch_size, train_set, test_set):\n",
        "    \n",
        "    trainloader = DataLoader(train_set, batch_size, shuffle=True)\n",
        "    testloader = DataLoader(test_set, batch_size, shuffle=False)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "  def train_epoch(self):  # Notice that all of the required arguments are now attributes!\n",
        "    self.net.train()\n",
        "\n",
        "    for X, y in self.trainloader:\n",
        "      y_hat = self.net(X)\n",
        "      local_loss = self.criterion(y_hat, y)\n",
        "\n",
        "      local_loss.backward()\n",
        "      self.optimizer.step()\n",
        "      self.optimizer.zero_grad()\n",
        "      \n",
        "    \n",
        "\n",
        "    return self.criterion(\n",
        "      self.net(self.trainloader.dataset.data),\n",
        "      self.trainloader.dataset.targets\n",
        "      )\n",
        "      \n",
        "  def test_epoch(self):\n",
        "    y = self.testloader.dataset.data\n",
        "    targets = self.testloader.dataset.targets\n",
        "    \n",
        "    y_hat = self.net(y)\n",
        "    mean_loss = self.criterion(y_hat, targets) # Asuuming reduction=\"mean\"\n",
        "    \n",
        "    target_class = torch.max(targets, 1)[1]\n",
        "    predicted_class = torch.max(y_hat, 1)[1] # Argmax gives predicted_class\n",
        "    \n",
        "    num_errors = len(torch.nonzero(predicted_class - target_class))\n",
        "    \n",
        "  \n",
        "    return mean_loss, num_errors\n",
        "  \n",
        "  def train_me(self):\n",
        "    for i in range(self.max_epoch):\n",
        "      epoch = i + 1\n",
        "      train_loss = self.train_epoch()\n",
        "      test_loss, test_err = self.test_epoch()\n",
        "      \n",
        "      print(f\"Epoch: {epoch} | Train Loss: {train_loss:.04} |\"\n",
        "            f\"Test Loss: {test_loss:.04} | Test Error: {test_err}\")\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ip2ilFPmBIi-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_7918/3452212340.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.data = torch.tensor(self.data, dtype=torch.float32)            # Casting to float to prevent future problems.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.5214, grad_fn=<DivBackward1>)\n",
            "Epoch: 1 | Train Loss: 2.079 |Test Loss: 2.087 | Test Error: 7647\n",
            "Epoch: 2 | Train Loss: 2.024 |Test Loss: 2.032 | Test Error: 7415\n",
            "Epoch: 3 | Train Loss: 1.959 |Test Loss: 1.973 | Test Error: 7108\n",
            "Epoch: 4 | Train Loss: 1.926 |Test Loss: 1.936 | Test Error: 6956\n",
            "Epoch: 5 | Train Loss: 1.906 |Test Loss: 1.918 | Test Error: 6863\n",
            "Epoch: 6 | Train Loss: 1.882 |Test Loss: 1.896 | Test Error: 6819\n",
            "Epoch: 7 | Train Loss: 1.869 |Test Loss: 1.884 | Test Error: 6717\n",
            "Epoch: 8 | Train Loss: 1.857 |Test Loss: 1.88 | Test Error: 6747\n",
            "Epoch: 9 | Train Loss: 1.835 |Test Loss: 1.858 | Test Error: 6612\n",
            "Epoch: 10 | Train Loss: 1.853 |Test Loss: 1.88 | Test Error: 6732\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def test_train_epoch(net=None):\n",
        "    mnist_net = Network(\n",
        "        dataset_type=\"mnist\",\n",
        "        width=16,\n",
        "        depth=2,\n",
        "        criterion=nn.CrossEntropyLoss(),\n",
        "        optimizer=optim.Adam,\n",
        "    )\n",
        "    print(mnist_net.train_epoch())\n",
        "\n",
        "    return mnist_net\n",
        "\n",
        "net = test_train_epoch()\n",
        "\n",
        "def test_test_epoch():\n",
        "    mnist_net = Network(\n",
        "    dataset_type=\"mnist\",\n",
        "    width=16,\n",
        "    depth=2,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=optim.Adam,\n",
        "    )\n",
        "    for i in range(10):\n",
        "        loss, err = mnist_net.test_epoch()\n",
        "        print(f\"Epoch: {i+1}, Mean loss: {loss}, Errors: {err}\")\n",
        "        \n",
        "        mnist_net.train_epoch()\n",
        "        \n",
        "    \n",
        "cifar_net = Network(\n",
        "    dataset_type=\"cifar\",\n",
        "    width=16,\n",
        "    depth=2,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=optim.Adam,\n",
        "    max_epoch=10\n",
        ")\n",
        "\n",
        "cifar_net.train_me()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oIG6rLvGYHLj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([0.5000, 0.6000, 0.7000, 0.9000]),\n",
            "indices=tensor([2, 1, 1, 0])) torch.return_types.max(\n",
            "values=tensor([1., 1., 1., 1.]),\n",
            "indices=tensor([0, 1, 0, 2]))\n",
            "\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.L1Loss(reduction=\"none\")\n",
        "\n",
        "y_hat = torch.tensor([\n",
        "    [0.2, 0., 0.5,],\n",
        "    [0.4, 0.6, 0.1,],\n",
        "    [0.2, 0.7, 0.5,],\n",
        "    [0.9, 0.6, 0.1,]\n",
        "])\n",
        "\n",
        "y = torch.tensor([\n",
        "    [1, 0., 0.,],\n",
        "    [0., 1, 0.,],\n",
        "    [1, 0., 0.,],\n",
        "    [0., 0, 1,]\n",
        "])\n",
        "print(torch.max(y_hat, 1), torch.max(y, 1))\n",
        "print()\n",
        "print(len(torch.nonzero((torch.max(y_hat, 1)[1] - torch.max(y, 1)[1]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7yjBjx2sVd"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.1: Implementations [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeE38cuv2sVd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "g5-RuWIO2sVd"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown. Please remember to comment the code and explain your reasoning. Include docstrings. Tutorial provide a good example of how to style your code.\n",
        "# Although not compulsory you could challenge yourself by using object oriented programming to structure your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sl5SVyf22sVd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'loading_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader, test_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mloading_data\u001b[49m(\u001b[38;5;241m30000\u001b[39m, train_set_mnist, test_set_mnist)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# for i, (j, k) in enumerate(train_dataloader):\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#   print(f\"i={i}\\n\\n\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#   print(f\"j={j.shape}\\n\\n\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#   print(f\"k={k}\\n\\n\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtargets\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loading_data' is not defined"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = loading_data(30000, train_set_mnist, test_set_mnist)\n",
        "# for i, (j, k) in enumerate(train_dataloader):\n",
        "#   print(f\"i={i}\\n\\n\")\n",
        "#   print(f\"j={j.shape}\\n\\n\")\n",
        "#   print(f\"k={k}\\n\\n\")\n",
        "\n",
        "train_dataloader.dataset.targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x5Kc96Z2sVd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8pc225D2sVd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHBQTtwc2sVd"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.2: Numerical exploration [2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLGvBWj2sVe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7sSjFlw2sVe"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ3YRvuo2sVe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PftX9KPq2sVn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzaGGVnT2sVn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMDhdlAQ2sVn"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyCTq6XN2sVo"
      },
      "source": [
        "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul5rk0ff2sVo"
      },
      "source": [
        "### Task 1: Proper weight scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NV1eU6d2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOdoN0OQ2sVo"
      },
      "source": [
        "### Task 2: Derive the GP relation for a single hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoIW9iH_2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbxrzInZ2sVo"
      },
      "source": [
        "### Task 3: Why in succession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oRi0kiP2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBiy5sKA2sVo"
      },
      "source": [
        "### Task 4: Derive the GP relation for multiple hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaIEL1AK2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNzWgsXt2sVo"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unW1bYqJ2sVo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FppSzjEZ2sVo"
      },
      "outputs": [],
      "source": [
        "# Please use float64 as default dtype for this part of the assignment\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# Another hint: when  computing [ K^L(X,X) + noise^2 Id ]^-1 y and  [ K^L(X,X) + noise^2 Id ]^-1 K^L(X,X*)\n",
        "# You can TRY cholesky solve as it should be p.d. (except case for numerical errors) - maybe you can use try:/except:\n",
        "# You can also try to enforce symmetry in posterior covariance by doing (K + K.t())/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_c3V9YH2sVp"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj_iVLGm2sVp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
