{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK8u_aGl2sVW"
      },
      "source": [
        "# Coursework 1 - Mathematics for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiWQAg9V2sVY"
      },
      "source": [
        "## CID: insert your CID here\n",
        "\n",
        "**Colab link:** insert colab link here\n",
        "\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YIqCr232sVZ"
      },
      "source": [
        "## Part 1: Quickfire questions [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbbcTvYZ2sVZ"
      },
      "source": [
        "#### Question 1 (True risk / Empirical risk):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aEJdGev2sVZ"
      },
      "source": [
        "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZcTQdB92sVZ"
      },
      "source": [
        "#### Question 3 (Dataset splitting):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHjZ21qA2sVZ"
      },
      "source": [
        "#### Question 4 (Occamâ€™s razor):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyVskoM22sVa"
      },
      "source": [
        "#### Question 5 (Generalisation error):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quzf--OL2sVa"
      },
      "source": [
        "#### Question 6 (Rademacher complexity pt1):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc414RJV2sVa"
      },
      "source": [
        "#### Question 7 (Rademacher complexity pt2):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY3o2MXl2sVa"
      },
      "source": [
        "#### Question 8 (Regularisation term in the loss function):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72DlajIW2sVb"
      },
      "source": [
        "#### Question 9 (Momentum gradient descent):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCwc9wkR2sVb"
      },
      "source": [
        "#### Question 10 (Adam):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXIZAndw2sVb"
      },
      "source": [
        "#### Question 11 (AdaGrad):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_k_g_qr2sVb"
      },
      "source": [
        "#### Question 12 (Decaying Learning Rate):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41dGi5wo2sVb"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 2: Short-ish proofs [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zlbVwj2sVb"
      },
      "source": [
        "\n",
        "### Question 2.1: Bounds on the risk [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDSxbRA2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp_Kr9T32sVb"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.2: On semi-definiteness [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2VWR9iQ2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4p6Fcyy2sVb"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.3: A quick recap of momentum [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1VFKHuZ2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwuPSbvP2sVc"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.4: Convergence proof [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNGw6ku2sVc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV-W82HD2sVc"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 3: A deeper dive into neural network implementations [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "qT6HJlNL2sVc"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uogO3SpP2sVc",
        "outputId": "f584ae90-5dfd-4784-f100-9410a12f2ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "File not found or corrupted.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[80], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m train_set_mnist \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                                          train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor()]));\n\u001b[1;32m      5\u001b[0m test_set_mnist \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m,download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                                         train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor()]),);\n\u001b[0;32m----> 8\u001b[0m train_set_cifar \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     11\u001b[0m test_set_cifar \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m,download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                                         train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor()]),);\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Normalizing data:\u001b[39;00m\n",
            "File \u001b[0;32m~/projects/maths_machine_learning_2024/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:65\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/projects/maths_machine_learning_2024/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:139\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/projects/maths_machine_learning_2024/.venv/lib/python3.10/site-packages/torchvision/datasets/utils.py:434\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    432\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 434\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/projects/maths_machine_learning_2024/.venv/lib/python3.10/site-packages/torchvision/datasets/utils.py:155\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# check integrity of downloaded file\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found or corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: File not found or corrupted."
          ]
        }
      ],
      "source": [
        "# Download datasets\n",
        "train_set_mnist = torchvision.datasets.MNIST(root=\"./data\", download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]));\n",
        "\n",
        "test_set_mnist = torchvision.datasets.MNIST(root=\"./data\",download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),);\n",
        "\n",
        "train_set_cifar = torchvision.datasets.CIFAR10(root=\"./data\", download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]));\n",
        "\n",
        "test_set_cifar = torchvision.datasets.CIFAR10(root=\"./data\",download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),);\n",
        "\n",
        "# Normalizing data:\n",
        "train_set_mnist.data = nn.functional.normalize(train_set_mnist.data.to(float), p=1)\n",
        "test_set_mnist.data = nn.functional.normalize(test_set_mnist.data.to(float), p=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NormalizedDataSet(Dataset):\n",
        "    def __init__(self, dataset_type, train: bool, normalize: bool, nclasses=10):\n",
        "        match (dataset_type, train):\n",
        "            case (\"mnist\", True):\n",
        "                self.data =  train_set_mnist.data\n",
        "                raw_targets = train_set_mnist.targets\n",
        "                \n",
        "            case (\"mnist\", False):\n",
        "                self.data =  test_set_mnist.data\n",
        "                raw_targets = test_set_mnist.targets\n",
        "\n",
        "            case (\"cifar\", True):\n",
        "                self.data =  train_set_cifar.data\n",
        "                raw_targets = train_set_cifar.targets\n",
        "                \n",
        "            case (\"cifar\", False):\n",
        "                self.data =  test_set_cifar.data\n",
        "                raw_targets = test_set_cifar.targets\n",
        "\n",
        "            case _:\n",
        "                raise ValueError(\"Dataset must be 'mnist' or 'cifar'\")\n",
        "            \n",
        "        if normalize and dataset_type==\"mnist\":                     # CIFAR is already normalized. I think.\n",
        "            self.data = nn.functional.normalize(self.data, p=1)\n",
        "            \n",
        "        self.targets = torch.zeros(len(raw_targets), nclasses) \n",
        "        for i, t in enumerate(raw_targets):\n",
        "            self.targets[i, int(t)] = 1.                            # Changing the targets into rows with 0 everywhere except the target\n",
        "\n",
        "        self.data = torch.tensor(self.data, dtype=torch.float32)            # Casting to float to prevent future problems.\n",
        "        self.targets = torch.tensor(self.targets, dtype=torch.float32)      # Just to make sure!                      \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.targets[index]\n",
        "            \n",
        "def test_dataset():\n",
        "    traindata = NormalizedDataSet(\"cifar\", False, True)\n",
        "    print(traindata[10])\n",
        "\n",
        "# test_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEsK7Qzo2sVd"
      },
      "outputs": [],
      "source": [
        "# Set seed\n",
        "SEED = int('02530622')\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GpBUZ0S2sVd",
        "outputId": "eb397d1d-82be-40b0-e2b5-c743e12a1602"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dim, nclass, width, depth):\n",
        "      super().__init__()\n",
        "      self.dim = dim\n",
        "      self.nclass = nclass\n",
        "      self.width = width\n",
        "      self.depth = depth\n",
        "      self.input_length = np.prod(dim)\n",
        "\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_in = nn.Linear(self.input_length, width)\n",
        "      self.linear_hidden = nn.Linear(width, width)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.linear_out = nn.Linear(width, nclass)\n",
        "\n",
        "    def forward(self, x):\n",
        "      flat_x = self.flatten(x)\n",
        "      lifted_x = self.linear_in(flat_x)\n",
        "\n",
        "      processed_x = lifted_x\n",
        "      for _ in range(self.depth):\n",
        "        processed_x = self.relu(self.linear_hidden(processed_x))\n",
        "\n",
        "      return self.linear_out(processed_x)\n",
        "\n",
        "def test_net(net=None):\n",
        "\n",
        "  mnist_net = Net((28, 28), 10, 16, 2) if net is None else net\n",
        "  sample_index = np.random.randint(10000)\n",
        "\n",
        "  x = train_set_mnist.data[sample_index, :, :]\n",
        "  x = torch.unsqueeze(x, 0)\n",
        "  print(mnist_net(x), train_set_mnist.targets[sample_index])\n",
        "\n",
        "\n",
        "# test_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h4H6KUOCu2-"
      },
      "outputs": [],
      "source": [
        "class Network():\n",
        "  def __init__(\n",
        "      self,\n",
        "      dataset_type,\n",
        "      width,\n",
        "      depth,\n",
        "      criterion,\n",
        "      optimizer,\n",
        "      batch_size=64,\n",
        "      lr=0.001,\n",
        "      max_epoch=1,\n",
        "      normalize=True\n",
        "      ):\n",
        "\n",
        "    match dataset_type:\n",
        "      case \"mnist\":\n",
        "        dim = (28, 28)\n",
        "        nclass = 10\n",
        "\n",
        "      case \"cifar\":\n",
        "        dim = (32, 32, 3)\n",
        "        nclass = 10\n",
        "\n",
        "      case _:\n",
        "        raise ValueError(\"Dataset must be 'mnist' or 'cifar'\")\n",
        "\n",
        "    self.trainset = NormalizedDataSet(dataset_type, train=True, normalize=normalize)\n",
        "    self.testset = NormalizedDataSet(dataset_type, train=False, normalize=normalize)\n",
        "    self.trainloader, self.testloader = self._loading_data(batch_size, self.trainset, self.testset)\n",
        "\n",
        "    self.net = Net(dim, nclass, width, depth)\n",
        "    \n",
        "    self.lr = lr\n",
        "    self.max_epoch = max_epoch\n",
        "    self.optimizer = optimizer(self.net.parameters(), lr=self.lr)\n",
        "    self.criterion = criterion\n",
        "\n",
        "\n",
        "  def _loading_data(self, batch_size, train_set, test_set):\n",
        "    \n",
        "    trainloader = DataLoader(train_set, batch_size, shuffle=True)\n",
        "    testloader = DataLoader(test_set, batch_size, shuffle=False)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "  def train_epoch(self):  # Notice that all of the required arguments are now attributes!\n",
        "    self.net.train()\n",
        "\n",
        "    for X, y in self.trainloader:\n",
        "      y_hat = self.net(X)\n",
        "      local_loss = self.criterion(y_hat, y)\n",
        "\n",
        "      local_loss.backward()\n",
        "      self.optimizer.step()\n",
        "      self.optimizer.zero_grad()\n",
        "      \n",
        "    \n",
        "\n",
        "    return self.criterion(\n",
        "      self.net(self.trainloader.dataset.data),\n",
        "      self.trainloader.dataset.targets\n",
        "      )\n",
        "      \n",
        "  def test_epoch(self):\n",
        "    print(self.testloader.dataset.data[500])\n",
        "    y_hat = self.net(self.testloader.dataset.data)\n",
        "    loss = self.criterion(y_hat, self.testloader.dataset.targets)\n",
        "    linear_loss_fn = nn.L1Loss(reduction=\"none\")\n",
        "    return linear_loss_fn(self.testloader.dataset.targets, y_hat)\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip2ilFPmBIi-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_train_epoch(net=None):\n",
        "    mnist_net = Network(\n",
        "        dataset_type=\"mnist\",\n",
        "        width=16,\n",
        "        depth=2,\n",
        "        criterion=nn.CrossEntropyLoss(),\n",
        "        optimizer=optim.Adam,\n",
        "    )\n",
        "    print(mnist_net.train_epoch())\n",
        "\n",
        "    return mnist_net\n",
        "\n",
        "net = test_train_epoch()\n",
        "\n",
        "def test_test_epoch():\n",
        "    mnist_net = Network(\n",
        "    dataset_type=\"mnist\",\n",
        "    width=16,\n",
        "    depth=2,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=optim.Adam,\n",
        "    )\n",
        "    for _ in range(2):\n",
        "        mnist_net.train_epoch()\n",
        "        \n",
        "    lin_loss = mnist_net.test_epoch()\n",
        "    \n",
        "    for i in lin_loss:\n",
        "        print(i)\n",
        "    \n",
        "test_test_epoch()\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIG6rLvGYHLj"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "y_hat = torch.tensor([\n",
        "    [0., 0., 1.,],\n",
        "    [0., 0., 0.,]\n",
        "])\n",
        "\n",
        "y = torch.tensor([\n",
        "    [0., 0., 0.,],\n",
        "    [0., 0., 0.,]\n",
        "])\n",
        "print()\n",
        "print(criterion(y, y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7yjBjx2sVd"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.1: Implementations [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeE38cuv2sVd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5-RuWIO2sVd"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown. Please remember to comment the code and explain your reasoning. Include docstrings. Tutorial provide a good example of how to style your code.\n",
        "# Although not compulsory you could challenge yourself by using object oriented programming to structure your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl5SVyf22sVd"
      },
      "outputs": [],
      "source": [
        "train_dataloader, test_dataloader = loading_data(30000, train_set_mnist, test_set_mnist)\n",
        "# for i, (j, k) in enumerate(train_dataloader):\n",
        "#   print(f\"i={i}\\n\\n\")\n",
        "#   print(f\"j={j.shape}\\n\\n\")\n",
        "#   print(f\"k={k}\\n\\n\")\n",
        "\n",
        "train_dataloader.dataset.targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x5Kc96Z2sVd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8pc225D2sVd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHBQTtwc2sVd"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.2: Numerical exploration [2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLGvBWj2sVe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7sSjFlw2sVe"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ3YRvuo2sVe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PftX9KPq2sVn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzaGGVnT2sVn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMDhdlAQ2sVn"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyCTq6XN2sVo"
      },
      "source": [
        "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul5rk0ff2sVo"
      },
      "source": [
        "### Task 1: Proper weight scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NV1eU6d2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOdoN0OQ2sVo"
      },
      "source": [
        "### Task 2: Derive the GP relation for a single hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoIW9iH_2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbxrzInZ2sVo"
      },
      "source": [
        "### Task 3: Why in succession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oRi0kiP2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBiy5sKA2sVo"
      },
      "source": [
        "### Task 4: Derive the GP relation for multiple hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaIEL1AK2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNzWgsXt2sVo"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unW1bYqJ2sVo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FppSzjEZ2sVo"
      },
      "outputs": [],
      "source": [
        "# Please use float64 as default dtype for this part of the assignment\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# Another hint: when  computing [ K^L(X,X) + noise^2 Id ]^-1 y and  [ K^L(X,X) + noise^2 Id ]^-1 K^L(X,X*)\n",
        "# You can TRY cholesky solve as it should be p.d. (except case for numerical errors) - maybe you can use try:/except:\n",
        "# You can also try to enforce symmetry in posterior covariance by doing (K + K.t())/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_c3V9YH2sVp"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj_iVLGm2sVp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
