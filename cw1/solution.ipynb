{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK8u_aGl2sVW"
      },
      "source": [
        "# Coursework 1 - Mathematics for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiWQAg9V2sVY"
      },
      "source": [
        "## CID: insert your CID here\n",
        "\n",
        "**Colab link:** insert colab link here\n",
        "\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YIqCr232sVZ"
      },
      "source": [
        "## Part 1: Quickfire questions [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbbcTvYZ2sVZ"
      },
      "source": [
        "#### Question 1 (True risk / Empirical risk):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aEJdGev2sVZ"
      },
      "source": [
        "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZcTQdB92sVZ"
      },
      "source": [
        "#### Question 3 (Dataset splitting):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHjZ21qA2sVZ"
      },
      "source": [
        "#### Question 4 (Occamâ€™s razor):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyVskoM22sVa"
      },
      "source": [
        "#### Question 5 (Generalisation error):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quzf--OL2sVa"
      },
      "source": [
        "#### Question 6 (Rademacher complexity pt1):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc414RJV2sVa"
      },
      "source": [
        "#### Question 7 (Rademacher complexity pt2):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY3o2MXl2sVa"
      },
      "source": [
        "#### Question 8 (Regularisation term in the loss function):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72DlajIW2sVb"
      },
      "source": [
        "#### Question 9 (Momentum gradient descent):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCwc9wkR2sVb"
      },
      "source": [
        "#### Question 10 (Adam):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXIZAndw2sVb"
      },
      "source": [
        "#### Question 11 (AdaGrad):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_k_g_qr2sVb"
      },
      "source": [
        "#### Question 12 (Decaying Learning Rate):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41dGi5wo2sVb"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 2: Short-ish proofs [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zlbVwj2sVb"
      },
      "source": [
        "\n",
        "### Question 2.1: Bounds on the risk [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDSxbRA2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp_Kr9T32sVb"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.2: On semi-definiteness [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2VWR9iQ2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4p6Fcyy2sVb"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.3: A quick recap of momentum [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1VFKHuZ2sVb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwuPSbvP2sVc"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.4: Convergence proof [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNGw6ku2sVc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV-W82HD2sVc"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 3: A deeper dive into neural network implementations [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qT6HJlNL2sVc"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uogO3SpP2sVc",
        "outputId": "f584ae90-5dfd-4784-f100-9410a12f2ebe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23.5%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100.0%\n",
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download datasets\n",
        "train_set_mnist = torchvision.datasets.MNIST(root=\"./data\", download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]));\n",
        "\n",
        "test_set_mnist = torchvision.datasets.MNIST(root=\"./data\",download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),);\n",
        "\n",
        "train_set_cifar = torchvision.datasets.CIFAR10(root=\"./data\", download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]));\n",
        "\n",
        "test_set_cifar = torchvision.datasets.CIFAR10(root=\"./data\",download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),);\n",
        "\n",
        "# Normalizing data:\n",
        "train_set_mnist.data = nn.functional.normalize(train_set_mnist.data.to(float), p=1)\n",
        "test_set_mnist.data = nn.functional.normalize(test_set_mnist.data.to(float), p=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QEsK7Qzo2sVd"
      },
      "outputs": [],
      "source": [
        "# Set seed\n",
        "SEED = int('02530622')\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GpBUZ0S2sVd",
        "outputId": "eb397d1d-82be-40b0-e2b5-c743e12a1602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1383,  0.1269, -0.0216, -0.0671, -0.1375, -0.1405,  0.1336, -0.0630,\n",
            "         -0.1591,  0.2608]], grad_fn=<AddmmBackward0>) tensor(0)\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dim, nclass, width, depth):\n",
        "      super().__init__()\n",
        "      self.dim = dim\n",
        "      self.nclass = nclass\n",
        "      self.width = width\n",
        "      self.depth = depth\n",
        "      self.input_length = np.prod(dim)\n",
        "\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_in = nn.Linear(self.input_length, width)\n",
        "      self.linear_hidden = nn.Linear(width, width)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.linear_out = nn.Linear(width, nclass)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.to(dtype=torch.float32)\n",
        "      flat_x = self.flatten(x)\n",
        "      lifted_x = self.linear_in(flat_x)\n",
        "\n",
        "      processed_x = lifted_x\n",
        "      for _ in range(self.depth):\n",
        "        processed_x = self.relu(self.linear_hidden(processed_x))\n",
        "\n",
        "      return self.linear_out(processed_x)\n",
        "\n",
        "def test_net(net=None):\n",
        "\n",
        "  mnist_net = Net((28, 28), 10, 16, 2) if net is None else net\n",
        "  sample_index = np.random.randint(10000)\n",
        "\n",
        "  x = train_set_mnist.data[sample_index, :, :]\n",
        "  x = torch.unsqueeze(x, 0).to(dtype=torch.float32) # TODO: fix some shit w. the Dtype\n",
        "  print(mnist_net(x), train_set_mnist.targets[sample_index])\n",
        "\n",
        "\n",
        "test_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0h4H6KUOCu2-"
      },
      "outputs": [],
      "source": [
        "class Network():\n",
        "  def __init__(\n",
        "      self,\n",
        "      dataset,\n",
        "      width,\n",
        "      depth,\n",
        "      batch_size=64,\n",
        "      lr=0.001,\n",
        "      epoches=1,\n",
        "      normalize=True\n",
        "      ):\n",
        "\n",
        "\n",
        "    match dataset:\n",
        "      case \"mnist\":\n",
        "        dim = (28, 28)\n",
        "        nclass = 10\n",
        "        trainset = train_set_mnist\n",
        "        testset = test_set_mnist\n",
        "\n",
        "      case \"cifar\":\n",
        "        dim = (32, 32, 3)\n",
        "        nclass = 10\n",
        "        trainset = train_set_cifar\n",
        "        testset = test_set_cifar\n",
        "\n",
        "      case _:\n",
        "        raise ValueError(\"Dataset must be 'mnist' or 'cifar'\")\n",
        "\n",
        "\n",
        "    self.net = Net(dim, nclass, width, depth)\n",
        "    self.lr\n",
        "    (self.trainloader, self.testloader) = \\\n",
        "      self.loading_data(batch_size, trainset, testset)\n",
        "\n",
        "\n",
        "  def _loading_data(self, batch_size, train_set, test_set): # TODO: add normalization, fix dty\n",
        "\n",
        "    trainloader = DataLoader(train_set, batch_size, shuffle=True)\n",
        "    testloader = DataLoader(test_set, batch_size, shuffle=False)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "  def train_epoch(self, trainloader, net, optimizer, criterion): #TODO add normalization!!!!\n",
        "      size = len(trainloader.dataset)\n",
        "      batch_size = trainloader.batch_size\n",
        "      net.train()\n",
        "\n",
        "      for batch, (X, y) in enumerate(trainloader):\n",
        "        # y = torch.tensor([ [0. if i!=target else 1. for i in range(10)] for target in y]) # TODO: Make preperation of targets outside and more general!!\n",
        "\n",
        "        y_hat = net(X)\n",
        "        local_loss = criterion(y_hat, y)\n",
        "\n",
        "        local_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #  new_targets = torch.tensor([[0. if i!=target else 1. for i in range(10)] for target in trainloader.dataset.targets])\n",
        "\n",
        "      return criterion(net(trainloader.dataset.data.to(dtype=torch.double)), trainloader.dataset.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ip2ilFPmBIi-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_train_epoch(net=None):\n",
        "    mnist_net = Net((28, 28), 10, 16, 2) if net is None else net\n",
        "    trainloader, testloader = loading_data(50, train_set_mnist, test_set_mnist)\n",
        "\n",
        "\n",
        "    train_epoch(\n",
        "        trainloader,\n",
        "        mnist_net,\n",
        "        optim.Adam(mnist_net.parameters(), 0.00025), # This combination seems quite good!\n",
        "        nn.CrossEntropyLoss()                 # TODO: some kind of normalization is a must!!!!\n",
        "        )\n",
        "\n",
        "    return mnist_net\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "zgX0kzd_SUzs",
        "outputId": "25598018-1b16-4abb-c4f0-cd1fc47df513"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'loading_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_net \u001b[38;5;241m=\u001b[39m \u001b[43mtest_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      3\u001b[0m   trained_net \u001b[38;5;241m=\u001b[39m test_train_epoch(trained_net)\n",
            "Cell \u001b[0;32mIn[22], line 3\u001b[0m, in \u001b[0;36mtest_train_epoch\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_train_epoch\u001b[39m(net\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     mnist_net \u001b[38;5;241m=\u001b[39m Net((\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m net \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m net\n\u001b[0;32m----> 3\u001b[0m     trainloader, testloader \u001b[38;5;241m=\u001b[39m \u001b[43mloading_data\u001b[49m(\u001b[38;5;241m50\u001b[39m, train_set_mnist, test_set_mnist)\n\u001b[1;32m      6\u001b[0m     train_epoch(\n\u001b[1;32m      7\u001b[0m         trainloader,\n\u001b[1;32m      8\u001b[0m         mnist_net,\n\u001b[1;32m      9\u001b[0m         optim\u001b[38;5;241m.\u001b[39mAdam(mnist_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.00025\u001b[39m), \u001b[38;5;66;03m# This combination seems quite good!\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()                 \u001b[38;5;66;03m# TODO: some kind of normalization is a must!!!!\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         )\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mnist_net\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loading_data' is not defined"
          ]
        }
      ],
      "source": [
        "trained_net = test_train_epoch()\n",
        "for _ in range(5):\n",
        "  trained_net = test_train_epoch(trained_net)\n",
        "\n",
        "for _ in range(10):test_net(trained_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIG6rLvGYHLj"
      },
      "outputs": [],
      "source": [
        "criterion = nn.L1Loss(reduction=\"sum\")\n",
        "\n",
        "y_hat = torch.tensor([\n",
        "    [0., 0., 1.],\n",
        "    [1., 0., 0.]\n",
        "])\n",
        "\n",
        "y = torch.tensor([\n",
        "    [0., 0., 0.],\n",
        "    [0., 0., 0.]\n",
        "])\n",
        "\n",
        "print(criterion(y, y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7yjBjx2sVd"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.1: Implementations [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeE38cuv2sVd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5-RuWIO2sVd"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown. Please remember to comment the code and explain your reasoning. Include docstrings. Tutorial provide a good example of how to style your code.\n",
        "# Although not compulsory you could challenge yourself by using object oriented programming to structure your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl5SVyf22sVd"
      },
      "outputs": [],
      "source": [
        "train_dataloader, test_dataloader = loading_data(30000, train_set_mnist, test_set_mnist)\n",
        "# for i, (j, k) in enumerate(train_dataloader):\n",
        "#   print(f\"i={i}\\n\\n\")\n",
        "#   print(f\"j={j.shape}\\n\\n\")\n",
        "#   print(f\"k={k}\\n\\n\")\n",
        "\n",
        "train_dataloader.dataset.targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x5Kc96Z2sVd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8pc225D2sVd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHBQTtwc2sVd"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.2: Numerical exploration [2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLGvBWj2sVe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7sSjFlw2sVe"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ3YRvuo2sVe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PftX9KPq2sVn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzaGGVnT2sVn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMDhdlAQ2sVn"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyCTq6XN2sVo"
      },
      "source": [
        "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul5rk0ff2sVo"
      },
      "source": [
        "### Task 1: Proper weight scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NV1eU6d2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOdoN0OQ2sVo"
      },
      "source": [
        "### Task 2: Derive the GP relation for a single hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoIW9iH_2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbxrzInZ2sVo"
      },
      "source": [
        "### Task 3: Why in succession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oRi0kiP2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBiy5sKA2sVo"
      },
      "source": [
        "### Task 4: Derive the GP relation for multiple hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaIEL1AK2sVo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNzWgsXt2sVo"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unW1bYqJ2sVo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FppSzjEZ2sVo"
      },
      "outputs": [],
      "source": [
        "# Please use float64 as default dtype for this part of the assignment\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# Another hint: when  computing [ K^L(X,X) + noise^2 Id ]^-1 y and  [ K^L(X,X) + noise^2 Id ]^-1 K^L(X,X*)\n",
        "# You can TRY cholesky solve as it should be p.d. (except case for numerical errors) - maybe you can use try:/except:\n",
        "# You can also try to enforce symmetry in posterior covariance by doing (K + K.t())/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_c3V9YH2sVp"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj_iVLGm2sVp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
